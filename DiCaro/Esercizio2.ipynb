{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 2\n",
    "Cercare synset di WordNet corretto per un insieme di definizioni"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMPORT AND COSTANTS\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import statistics\n",
    "import pandas as pd\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "file_path = 'TLN-definitions-23.tsv'\n",
    "RELEVANT_WORD_SIZE_FOR_GENUS = 5\n",
    "MIN_SYNSET_HEIGHT = 2\n",
    "MEANING_CANDIDATES_SIZE = 5\n",
    "DEVIATION = 4 #Used for refine dataset\n",
    "#Global variable used for dynamic programming\n",
    "meaningCandidates = []"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utilities functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#Function used to load the input data\n",
    "def parse_tsv_file(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    return df\n",
    "\n",
    "#Given a list of sentences calculate the avarage sentence lenght\n",
    "def average_length(dataset):\n",
    "    lengths = [len(sentence) for sentence in dataset]\n",
    "    avg_length = statistics.mean(lengths)\n",
    "    return avg_length\n",
    "\n",
    "#Lemmatize sentences and set all verb to lemma to avoid different verbal forms\n",
    "def lemmatized_tokens(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmas = []\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        if token.isalpha() and token not in stop_words:\n",
    "            if tag.startswith('VB'):\n",
    "                lemmas.append(lemmatizer.lemmatize(token, pos='v'))\n",
    "            else:\n",
    "                lemmas.append(lemmatizer.lemmatize(token))\n",
    "    return lemmas\n",
    "\n",
    "#Function used to create the dictionary structure used to store definitions in an organized way from the csv dataset\n",
    "def elaborate_dataset(dataframe):\n",
    "    dataset = {\n",
    "        'door': [],\n",
    "        'ladybug': [],\n",
    "        'pain': [],\n",
    "        'blurriness': []\n",
    "    }\n",
    "    dataframe = dataframe.iloc[:, 1:]  # Rimuovi la prima colona che non serve\n",
    "    for index, row in dataframe.iterrows():\n",
    "        for column in dataframe.columns:\n",
    "            dataset[column].extend([lemmatized_tokens(row[column])])\n",
    "    return dataset\n",
    "\n",
    "#Remove from the dataset all sentences that differ more than k  from the average of length\n",
    "def refine_dataset(dataset,k):\n",
    "    for key in dataset:\n",
    "        avg = int(average_length(dataset[key]))\n",
    "        dataset[key] = [elem for elem in  dataset[key] if abs(len(elem)-avg) <= k]\n",
    "    return dataset\n",
    "\n",
    "#Tokenize sentences and set all verb to lemma to avoid different verbal forms\n",
    "def lemmatized_tokens(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmas = []\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        if token.isalpha() and token not in stop_words:\n",
    "            if tag.startswith('VB'):\n",
    "                lemmas.append(lemmatizer.lemmatize(token, pos='v'))\n",
    "            else:\n",
    "                lemmas.append(lemmatizer.lemmatize(token))\n",
    "    return lemmas\n",
    "\n",
    "#Function used to clear the global array\n",
    "def clear_candidate():\n",
    "    while (len(meaningCandidates) > 0):\n",
    "        meaningCandidates.pop()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similarity function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#Use calculate the word intersection between two sentences,used to perform the lexical overlap\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "#Calculate avarage similarity between a synset definitions and the list of definitions from the dataset\n",
    "def calc_similarity(definition, definitions_dataset):\n",
    "    result = 0\n",
    "    for sentence in definitions_dataset:\n",
    "        #Normalize for the avarage of the definition and the sentence to avoid bias for short defined synsets\n",
    "        result += len(intersection(sentence, definition)) / ((len(sentence) + len(definition)) / 2)\n",
    "    return result / (len(definitions_dataset))\n",
    "\n",
    "\n",
    "\n",
    "#This recursive function calculates the similarity between all the synsets in a wordnet tree with head synset with the definitions of a term.\n",
    "#The result is the global variable meaningCandidates used to store tuples (Name,definitions,similarity)\n",
    "#In this way in each point of the recursive process we can easyli find if a synset similarity has already been calculated and avoid expensive operations\n",
    "def calcSimilarityForSynset(synset, sentences):\n",
    "    #If this synset isn't present in the result structure\n",
    "    if not any(synset.name() == item[0] for item in meaningCandidates):\n",
    "        #Calculate similarity and add it\n",
    "        similarity = calc_similarity(lemmatized_tokens(synset.definition()), sentences)\n",
    "        meaningCandidates.append((synset.name(), synset.definition(), similarity))\n",
    "\n",
    "    #For each child synset\n",
    "    for hyponim in synset.hyponyms():\n",
    "        #If the child synset isn't present in the result list we calculate the similarity of it's subtree\n",
    "        if not any(hyponim.name() == item[0] for item in meaningCandidates):\n",
    "            calcSimilarityForSynset(hyponim, sentences)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Synset elaboration functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Given a dataset we extract the list of all the words present in the dataset sentences ordered by their frequencies\n",
    "def getWordsInOrder(dataset):\n",
    "    print(\"ELABORATING WORDS FROM DATASET ON \", len(dataset), \" SENTENCES\")\n",
    "    words_dict = {}\n",
    "    #For each sentence in the dataset\n",
    "    for sentence in dataset:\n",
    "        #For each word in each sentence\n",
    "        for word in sentence:\n",
    "            #Insert/update the word count\n",
    "            if (word in words_dict):\n",
    "                words_dict[word] += 1\n",
    "            else:\n",
    "                words_dict[word] = 1\n",
    "    #Transform the dictionary in a tuple list and sort it by the frequency\n",
    "    my_list = list(words_dict.items())\n",
    "    sorted_list = sorted(my_list, key=lambda x: x[1], reverse=True)\n",
    "    print(\"ELABORATED WORDS, SHOWING FIRST\", RELEVANT_WORD_SIZE_FOR_GENUS, \" RELEVANT WORDS FIND ON DESCRIPTIONS\")\n",
    "    #Select only TOP RELEVANT_WORD_SIZE_FOR_GENUS\n",
    "    for item in sorted_list[0:RELEVANT_WORD_SIZE_FOR_GENUS]:\n",
    "        print(item)\n",
    "    return sorted_list\n",
    "\n",
    "#Given a list of words extract all synset for each word\n",
    "def getSynsetsInOrderFromWordNet(words):\n",
    "    print(\"ELABORATING SYNSET SEARCH ON \", len(words), \" WORDS\")\n",
    "    synsetWithHeight = []\n",
    "    #For each word\n",
    "    for word in words:\n",
    "        #Load all word synsets\n",
    "        synsets = wordnet.synsets(word[0])\n",
    "        #For each synset\n",
    "        for synset in synsets:\n",
    "            #If it isn't too high in the gerarchy we save it\n",
    "            if synset.max_depth() > MIN_SYNSET_HEIGHT:\n",
    "                synsetWithHeight.append((synset.name(), synset.max_depth(), synset))\n",
    "    #Sort all synset based on synset height so that we explore first the deepest synset to use dynamic programming\n",
    "    sortedSynsetWithHeight = sorted(synsetWithHeight, key=lambda x: x[1])\n",
    "    print(\"FOUND A TOTAL OF \", len(sortedSynsetWithHeight), \" SYNSETS \")\n",
    "    return sortedSynsetWithHeight\n",
    "\n",
    "#Extract from the list of all the synsets of the most relevant words in the definitions and the list of definitions a list of possible synsets\n",
    "def getMeaningCandidatesFromSynsets(synsets, sentences):\n",
    "    print(\"ELABORATING MEANING ON \", len(synsets), \" WITH A TOTAL OF \", len(sentences), \" DEFINITIONS\")\n",
    "    #For each synset\n",
    "    for synset in synsets:\n",
    "        #Calculate the similarity between the synset and the sentences\n",
    "        #The result of this calculation is stored in the global variable meaningCandidates as explained in the function definition\n",
    "        calcSimilarityForSynset(synset[2], sentences)\n",
    "    #Sort the meaning candidates by their similarity\n",
    "    sortedmeaningCandidates = sorted(meaningCandidates, key=lambda x: x[2], reverse=True)\n",
    "    print(\"ELABORATED MEANING, SHOWING FIRST \", MEANING_CANDIDATES_SIZE, \" RESULTS WITH SCORES: \")\n",
    "    for item in sortedmeaningCandidates[0:MEANING_CANDIDATES_SIZE]:\n",
    "        print(item)\n",
    "    return sortedmeaningCandidates[0:MEANING_CANDIDATES_SIZE]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df = parse_tsv_file(file_path)\n",
    "dataset = elaborate_dataset(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Elaborate meaning for word Door"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELABORATING DOOR\n",
      "ELABORATING WORDS FROM DATASET ON  30  SENTENCES\n",
      "ELABORATED WORDS, SHOWING FIRST 5  RELEVANT WORDS FIND ON DESCRIPTIONS\n",
      "('room', 14)\n",
      "('object', 14)\n",
      "('access', 11)\n",
      "('open', 10)\n",
      "('allow', 9)\n",
      "ELABORATING SYNSET SEARCH ON  5  WORDS\n",
      "FOUND A TOTAL OF  27  SYNSETS \n",
      "ELABORATING MEANING ON  27  WITH A TOTAL OF  30  DEFINITIONS\n",
      "ELABORATED MEANING, SHOWING FIRST  5  RESULTS WITH SCORES: \n",
      "('doorway.n.01', 'the entrance (the space in a wall) through which you enter or leave a room or building; the space that a door can close', 0.1772477982213478)\n",
      "('dining_room.n.01', 'a room used for dining', 0.1336919561919562)\n",
      "('anteroom.n.01', 'a large entrance or reception room or area', 0.1231687897448767)\n",
      "('room.n.04', 'the people who are present in a room', 0.12072125042713279)\n",
      "('bedroom.n.01', 'a room used primarily for sleeping', 0.11999106066468496)\n"
     ]
    }
   ],
   "source": [
    "print(\"ELABORATING DOOR\")\n",
    "\n",
    "doorWords = getWordsInOrder(dataset['door'])\n",
    "doorParentSynsetCandidates = getSynsetsInOrderFromWordNet(doorWords[0:RELEVANT_WORD_SIZE_FOR_GENUS])\n",
    "doorMeaningCandidates = getMeaningCandidatesFromSynsets(doorParentSynsetCandidates, dataset['door'])\n",
    "clear_candidate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Elaborate meaning for word Ladybug"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELABORATING LADYBUG\n",
      "ELABORATING WORDS FROM DATASET ON  30  SENTENCES\n",
      "ELABORATED WORDS, SHOWING FIRST 5  RELEVANT WORDS FIND ON DESCRIPTIONS\n",
      "('insect', 28)\n",
      "('red', 26)\n",
      "('black', 22)\n",
      "('small', 20)\n",
      "('dot', 18)\n",
      "ELABORATING SYNSET SEARCH ON  5  WORDS\n",
      "FOUND A TOTAL OF  20  SYNSETS \n",
      "ELABORATING MEANING ON  20  WITH A TOTAL OF  30  DEFINITIONS\n",
      "ELABORATED MEANING, SHOWING FIRST  5  RESULTS WITH SCORES: \n",
      "('buffalo_carpet_beetle.n.01', 'a small black and red and white carpet beetle', 0.3372284554136081)\n",
      "('aphid.n.01', 'any of various small plant-sucking insects', 0.3226517926517927)\n",
      "('two-spotted_ladybug.n.01', 'red ladybug with a black spot on each wing', 0.29553888377417786)\n",
      "('leaf_bug.n.01', 'small bright-colored insect that feeds on plant juices', 0.265823979941627)\n",
      "('chinch_bug.n.01', 'small black-and-white insect that feeds on cereal grasses', 0.265823979941627)\n"
     ]
    }
   ],
   "source": [
    "print(\"ELABORATING LADYBUG\")\n",
    "\n",
    "ladyBugWords = getWordsInOrder(dataset['ladybug'])\n",
    "ladyBugParentSynsetCandidates = getSynsetsInOrderFromWordNet(ladyBugWords[0:RELEVANT_WORD_SIZE_FOR_GENUS])\n",
    "ladyBugMeaningCandidates = getMeaningCandidatesFromSynsets(ladyBugParentSynsetCandidates, dataset['ladybug'])\n",
    "clear_candidate()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Elaborate meaning for word Pain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELABORATING PAIN\n",
      "ELABORATING WORDS FROM DATASET ON  30  SENTENCES\n",
      "ELABORATED WORDS, SHOWING FIRST 5  RELEVANT WORDS FIND ON DESCRIPTIONS\n",
      "('physical', 14)\n",
      "('feeling', 11)\n",
      "('emotional', 10)\n",
      "('sensation', 10)\n",
      "('cause', 6)\n",
      "ELABORATING SYNSET SEARCH ON  5  WORDS\n",
      "FOUND A TOTAL OF  17  SYNSETS \n",
      "ELABORATING MEANING ON  17  WITH A TOTAL OF  30  DEFINITIONS\n",
      "ELABORATED MEANING, SHOWING FIRST  5  RESULTS WITH SCORES: \n",
      "('feeling.n.04', 'a physical sensation that you experience', 0.2068701668701669)\n",
      "('suffering.n.04', 'feelings of mental or physical pain', 0.20522015022015025)\n",
      "('agony.n.01', 'intense feelings of suffering; acute mental or physical pain', 0.16490142210730444)\n",
      "('wildness.n.01', 'a feeling of extreme emotional intensity', 0.15920061420061418)\n",
      "('sensitivity.n.03', 'sensitivity to emotional feelings (of self and others)', 0.14241462241462244)\n"
     ]
    }
   ],
   "source": [
    "print(\"ELABORATING PAIN\")\n",
    "\n",
    "painWords = getWordsInOrder(dataset['pain'])\n",
    "painParentSynsetCandidates = getSynsetsInOrderFromWordNet(painWords[0:RELEVANT_WORD_SIZE_FOR_GENUS])\n",
    "painMeaningCandidates = getMeaningCandidatesFromSynsets(painParentSynsetCandidates, dataset['pain'])\n",
    "clear_candidate()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Elaborate meaning for word Blurriness"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELABORATING BLURRINESS\n",
      "ELABORATING WORDS FROM DATASET ON  30  SENTENCES\n",
      "ELABORATED WORDS, SHOWING FIRST 5  RELEVANT WORDS FIND ON DESCRIPTIONS\n",
      "('see', 8)\n",
      "('image', 6)\n",
      "('visual', 6)\n",
      "('something', 5)\n",
      "('eye', 5)\n",
      "ELABORATING SYNSET SEARCH ON  5  WORDS\n",
      "FOUND A TOTAL OF  26  SYNSETS \n",
      "ELABORATING MEANING ON  26  WITH A TOTAL OF  30  DEFINITIONS\n",
      "ELABORATED MEANING, SHOWING FIRST  5  RESULTS WITH SCORES: \n",
      "('memory_picture.n.01', 'a memory image that is similar to a visual perception', 0.0833507817718344)\n",
      "('visual_image.n.01', 'a mental image that is similar to a visual perception', 0.07779522621627884)\n",
      "('mental_picture.n.01', 'a clear and telling mental image', 0.055752765752765755)\n",
      "('naked_eye.n.01', 'the eye unaided by any optical instrument that alters the power of vision or alters the apparent size or distance of objects', 0.05455394889033331)\n",
      "('memory_image.n.01', 'a mental image of something previously experienced', 0.05365183148464882)\n"
     ]
    }
   ],
   "source": [
    "print(\"ELABORATING BLURRINESS\")\n",
    "\n",
    "blurrinessBugWords = getWordsInOrder(dataset['blurriness'])\n",
    "blurrinessParentSynsetCandidates = getSynsetsInOrderFromWordNet(blurrinessBugWords[0:RELEVANT_WORD_SIZE_FOR_GENUS])\n",
    "blurrinessMeaningCandidates = getMeaningCandidatesFromSynsets(blurrinessParentSynsetCandidates,\n",
    "                                                              dataset['blurriness'])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
