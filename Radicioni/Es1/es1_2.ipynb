{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Implementare lesk e verificarne accuratezza tramite SemCor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Import e costanti\n",
    "import csv\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "NUM_TESTS = 10\n",
    "TEST_SIZE = 50\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('pattern')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Utilities\n",
    "\n",
    "#Calcola intersezione tra due liste\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "#Funziona che lemmatizza una frase e rimuove le stop words\n",
    "def lemmatized_tokens(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmas = []\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        if token.isalpha() and token not in stop_words:\n",
    "            if tag.startswith('VB'):\n",
    "                lemmas.append(lemmatizer.lemmatize(token, pos='v'))\n",
    "            else:\n",
    "                lemmas.append(lemmatizer.lemmatize(token))\n",
    "    return lemmas\n",
    "#Parsifichiamo il file di gold_key\n",
    "def parse_gold_key(filename):\n",
    "    gold_key = {}\n",
    "    with open(filename, 'r') as csv_file:\n",
    "        # Create a CSV reader with space as the delimiter\n",
    "        reader = csv.reader(csv_file, delimiter=' ')\n",
    "\n",
    "        # Iterate over each row in the CSV\n",
    "        for row in reader:\n",
    "            wn_identifier = row[1]\n",
    "            for i in range(0, len(wn_identifier)):\n",
    "                if (wn_identifier[i] == ':'):\n",
    "                    wn_identifier = wn_identifier[0:i]\n",
    "                    break\n",
    "            wn_identifier_1 = wn_identifier.replace(\"%\", \".n.0\")\n",
    "            wn_identifier_2 = wn_identifier.replace(\"%\", \".v.0\")\n",
    "            wn_identifier_3 = wn_identifier.replace(\"%\", \".r.0\")\n",
    "            wn_identifier_3 = wn_identifier.replace(\"%\", \".a.0\")\n",
    "            gold_key[row[0]] = (wn_identifier_1, wn_identifier_2, wn_identifier_3)\n",
    "\n",
    "    return gold_key\n",
    "#Parsifichiamo xml di input\n",
    "def parse_xml_to_list(xml_file):\n",
    "    dataset = []\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    for child in root:\n",
    "        for element in child:\n",
    "            sublist = []\n",
    "            if (element.tag == \"sentence\"):\n",
    "                for word in element:\n",
    "                    sublist.append((word.text, word.get(\"pos\"), word.get(\"lemma\"), word.get(\"id\")))\n",
    "                dataset.append(sublist)\n",
    "\n",
    "    return dataset\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lesk Algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Algoritmo Lesk implementato a mano\n",
    "def lesk_algorithm(term, sentence):\n",
    "    #Variabili per salvare il massimo\n",
    "    max_overlap = -1\n",
    "    best_synset = None\n",
    "    #Estraggo i possibili synset\n",
    "    synsets = wordnet.synsets(term[2])\n",
    "    #Per ogni possibile synset\n",
    "    for synset in synsets:\n",
    "        synset_overlap = 0\n",
    "        #Lemmatizzo la definizione e aggiungo anche eventuali esempi se disponibili\n",
    "        definition = lemmatized_tokens(synset.definition())\n",
    "        if (synset.examples()):\n",
    "            for example in synset.examples():\n",
    "                definition += lemmatized_tokens(example)\n",
    "        # Lemmatizzo la frase ricostruendo la stringa per rimuovere anche le stop words\n",
    "        string_sentence = \"\"\n",
    "        for word in sentence:\n",
    "            string_sentence += ' ' + word[0]\n",
    "        string_sentence = lemmatized_tokens(string_sentence)\n",
    "        #Calcolo l'overlap come la lunghezza dell'intersezione dei termini della definizione e della frase di partenza\n",
    "        synset_overlap = (len(intersection(definition, string_sentence)))\n",
    "        #Aggiorno se trovo un nuovo massimo\n",
    "        if (synset_overlap > max_overlap):\n",
    "            max_overlap = synset_overlap\n",
    "            best_synset = synset\n",
    "    return best_synset\n",
    "\n",
    "#Algoritmo Lesk di Ntlk, usato per testare le performance\n",
    "def apply_state_of_art_lesk(term, sentence):\n",
    "    string_sentence = \"\"\n",
    "    for word in sentence:\n",
    "        string_sentence += ' ' + word[0]\n",
    "    tokens = string_sentence.split()\n",
    "\n",
    "    return lesk(tokens, term[2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main con esecuzione dei test estraendo frasi casuali"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random.seed(22)\n",
    "#Carico i dati\n",
    "dataset = parse_xml_to_list(\"WSD_Training_Corpora/SemCor/semcor.data.xml\")\n",
    "gold_key = parse_gold_key(\"WSD_Training_Corpora/SemCor/semcor.gold.key.txt\")\n",
    "\n",
    "#Eseguo NUM_TESTS differenti Test\n",
    "for i in range(NUM_TESTS):\n",
    "    n_tests = 0\n",
    "    #Numero di test corretti per l'algoritmo Lesk fatto a mano\n",
    "    n_hits = 0\n",
    "    #Numero di test corretti per l'algoritmo Lesk fatto da nltk\n",
    "    n_state_of_art_hits = 0\n",
    "    #Estraggo dal dataset TEST_SIZE possibili frasi\n",
    "    sample = random.sample(dataset, TEST_SIZE)\n",
    "    #Per ogni frase\n",
    "    for sentence in sample:\n",
    "        #Estraggo tutti i termini che hanno un synset di WordNet associato e quindi sono disambiguabili\n",
    "        searchable_terms = [t for t in sentence if t[3] is not None]\n",
    "        if (len(searchable_terms) > 0):\n",
    "            #Seleziono un termine a caso tra quelli disponibili\n",
    "            term = random.sample(searchable_terms, 1)[0]\n",
    "\n",
    "            #Calcolo il synset con lesk fatto a mano\n",
    "            proposed_synset = lesk_algorithm(term, sentence)\n",
    "            n_tests += 1\n",
    "            #Verifico correttezza\n",
    "            if proposed_synset is not None:\n",
    "                if proposed_synset.name() in gold_key[term[3]]:\n",
    "                    n_hits += 1\n",
    "            #Calcolo il synset con lesk ntlk\n",
    "            state_of_art_synset = apply_state_of_art_lesk(term, sentence)\n",
    "            #Verifico correttezza\n",
    "            if state_of_art_synset is not None:\n",
    "                if state_of_art_synset.name() in gold_key[term[3]]:\n",
    "                    n_state_of_art_hits += 1\n",
    "\n",
    "    #Calcolo accuratezze e stampo i risultati\n",
    "    acc = (n_hits / n_tests) * 100\n",
    "    acc_state_of_art = (n_state_of_art_hits / n_tests) * 100\n",
    "    print(\"Accuracy from our lesk on test\", i, \":\", acc, \"%\")\n",
    "    print(\"Accuracy for nltk lesk on test\", i, \":\", acc_state_of_art, \"%\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
