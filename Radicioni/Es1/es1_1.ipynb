{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Implementare misure di similarità basate su WordNet e calcolare indici di correlazione"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import e costanti\n",
    "import csv\n",
    "import math\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "WN_3_1_MAX_DEPTH=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#Utilities\n",
    "def read_csv(filename):\n",
    "    dataset=[]\n",
    "    with open(filename, 'r') as file:\n",
    "        # Create a CSV reader object\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            row[2]=float(row[2])\n",
    "            dataset.append(row+[None,None,None])\n",
    "    return dataset\n",
    "\n",
    "def normalize(value, min_val, max_val):\n",
    "    return (value - min_val) / (max_val - min_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Misura di similarità di Wu & Palmer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def calculate_wu_palmer(dataset):\n",
    "    for row in dataset:\n",
    "        #Le righe del dataset sono array di due elementi\n",
    "        #Estraggo i synset dei termini\n",
    "        first_elem_synset=wordnet.synsets(row[0])\n",
    "        second_elem_synset = wordnet.synsets(row[1])\n",
    "        #Costante per salvare la massima similarità tra elementi\n",
    "        max=-1\n",
    "        #Provo tutte le combinazioni di synset\n",
    "        for first_synset in first_elem_synset:\n",
    "            for second_synset in second_elem_synset:\n",
    "                first_elem_height=first_synset.max_depth()+1\n",
    "                second_elem_height=second_synset.max_depth()+1\n",
    "                break_loop=True\n",
    "                #While per la ricerca del del Lowest Common Subsumer\n",
    "                #Uso una variabile loop perchè per alcune coppie si rischia di non trovare il valore quindi gestiamo la casistica\n",
    "                while(break_loop):\n",
    "                    if(first_synset == second_synset):\n",
    "                     break_loop=False\n",
    "                    #Se non abbiamo trovato il LCS faccio salire il synset più profondo\n",
    "                    elif(first_synset.max_depth() > second_synset.max_depth()):\n",
    "                        hyper=first_synset.hypernyms()\n",
    "                        if(len(hyper)>0):\n",
    "                            first_synset = hyper[0]\n",
    "                        else:\n",
    "                            break_loop=False\n",
    "                            first_elem_height=-1\n",
    "                    else:\n",
    "                        hyper = second_synset.hypernyms()\n",
    "                        if (len(hyper) > 0):\n",
    "                            second_synset = hyper[0]\n",
    "                        else:\n",
    "                            break_loop = False\n",
    "                            second_elem_height = -1\n",
    "                lcs_height=first_synset.max_depth()+1\n",
    "                #Se l'LCS è stato trovato\n",
    "                if(first_elem_height>=0 and second_elem_height>=0):\n",
    "                    index=((2*lcs_height)/(first_elem_height+second_elem_height))*10\n",
    "                else:\n",
    "                    index=-1\n",
    "                #Index rappresenta la similarità, in caso sia il nuovo massimo lo aggiorno\n",
    "                if(index>max):\n",
    "                    max=index\n",
    "        #I massimi per ogni coppia vengono inseriti nella terza cella in modo da creare un dataset con tutte le similarità\n",
    "        row[3]=max\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Shortest Path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def calculate_shortest_path(dataset):\n",
    "    for row in dataset:\n",
    "        #Le righe del dataset sono array di due elementi\n",
    "        #Estraggo i synset dei termini\n",
    "        first_elem_synset=wordnet.synsets(row[0])\n",
    "        second_elem_synset = wordnet.synsets(row[1])\n",
    "        max_depth = WN_3_1_MAX_DEPTH\n",
    "        #Costante per salvare la massima similarità tra elementi\n",
    "        max=-1\n",
    "        #Provo tutte le combinazioni di synset\n",
    "        for first_synset in first_elem_synset:\n",
    "            for second_synset in second_elem_synset:\n",
    "                #Steps rappresenta il numero di passi nel cammino da un synset all'altro\n",
    "                steps=0\n",
    "                #Uso while simile a ricerca del LCS segnando però il numero di hypernyms attraversato\n",
    "                #Uso una variabile loop perchè per alcune coppie si rischia di non trovare il valore quindi gestiamo la casistica\n",
    "                break_loop=True\n",
    "                while(break_loop):\n",
    "                    if(first_synset == second_synset):\n",
    "                     break_loop=False\n",
    "                    elif(first_synset.max_depth() > second_synset.max_depth()):\n",
    "                        hyper=first_synset.hypernyms()\n",
    "                        if(len(hyper)>0):\n",
    "                            first_synset = hyper[0]\n",
    "                        else:\n",
    "                            break_loop=False\n",
    "                            steps=-2\n",
    "                    else:\n",
    "                        hyper = second_synset.hypernyms()\n",
    "                        if (len(hyper) > 0):\n",
    "                            second_synset = hyper[0]\n",
    "                        else:\n",
    "                            break_loop = False\n",
    "                            steps = -2\n",
    "                    steps+=1\n",
    "                #Se è stato trovato un cammino\n",
    "                if(steps>=0):\n",
    "                    #Normalizzo per il valore massimo e moltiplico per 10 per avere un valore nel range [1,10]\n",
    "                    index=index=normalize(((2*max_depth)-steps),0,(2*max_depth))*10\n",
    "                else:\n",
    "                    index=-1\n",
    "                 #Index rappresenta la similarità, in caso sia il nuovo massimo lo aggiorno\n",
    "                if(index>max):\n",
    "                    max=index\n",
    "        #I massimi per ogni coppia vengono inseriti nella quarta cella in modo da creare un dataset con tutte le similarità\n",
    "        row[4]=max\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Leakcock & Chodorow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def calculate_leakcock_chodorow(dataset):\n",
    "    for row in dataset:\n",
    "        #Le righe del dataset sono array di due elementi\n",
    "        #Estraggo i synset dei termini\n",
    "        first_elem_synset=wordnet.synsets(row[0])\n",
    "        second_elem_synset = wordnet.synsets(row[1])\n",
    "        max_depth = WN_3_1_MAX_DEPTH\n",
    "        #Costante per salvare la massima similarità tra elementi\n",
    "        max = -1\n",
    "        #Provo tutte le combinazioni di synset\n",
    "        for first_synset in first_elem_synset:\n",
    "            for second_synset in second_elem_synset:\n",
    "                #Steps rappresenta il numero di passi nel cammino da un synset all'altro\n",
    "                steps=0\n",
    "                break_loop=True\n",
    "                #Uso while come in shortest path per trovare la lunghezza del cammino\n",
    "                #Uso una variabile loop perchè per alcune coppie si rischia di non trovare il valore quindi gestiamo la casistica\n",
    "                while(break_loop):\n",
    "                    if(first_synset == second_synset):\n",
    "                     break_loop=False\n",
    "                    elif(first_synset.max_depth() > second_synset.max_depth()):\n",
    "                        hyper=first_synset.hypernyms()\n",
    "                        if(len(hyper)>0):\n",
    "                            first_synset = hyper[0]\n",
    "                        else:\n",
    "                            break_loop=False\n",
    "                            steps=-2\n",
    "                    else:\n",
    "                        hyper = second_synset.hypernyms()\n",
    "                        if (len(hyper) > 0):\n",
    "                            second_synset = hyper[0]\n",
    "                        else:\n",
    "                            break_loop = False\n",
    "                            steps = -2\n",
    "                    steps+=1\n",
    "                #Se è stato trovato un cammino\n",
    "                if(steps>=0):\n",
    "                    #Normalizzo per il valore massimo e moltiplico per 10 per avere un valore nel range [1,10]\n",
    "                    index=normalize((-1*math.log((steps+1)/((2*max_depth)+1))),0,math.log(2*max_depth+1))*10\n",
    "                else:\n",
    "                    index=-1\n",
    "                #Index rappresenta la similarità, in caso sia il nuovo massimo lo aggiorno\n",
    "                if(index>max):\n",
    "                    max=index\n",
    "        #I massimi per ogni coppia vengono inseriti nella quinta cella in modo da creare un dataset con tutte le similarità\n",
    "        row[5]=max\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main e calcolo similarità e indici"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perason for Wu Palmer:  0.20150625818011983\n",
      "Spearman rank for Wu Palmer:  0.21943244102990547 \n",
      "\n",
      "Perason for Shortest Path:  -0.022333216754054395\n",
      "Spearman rank for Shortest Path:  0.16174828058822424 \n",
      "\n",
      "Perason for Leakcock & Chodorow:  0.10251688106296773\n",
      "Spearman rank for Leakcock & Chodorow:  0.16174828058822424 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Carico il dataset\n",
    "dataset=read_csv(\"WordSim353/WordSim353/WordSim353.csv\")\n",
    "\n",
    "#Calcolo similarità\n",
    "dataset=calculate_wu_palmer(dataset)\n",
    "dataset=calculate_shortest_path(dataset)\n",
    "dataset=calculate_leakcock_chodorow(dataset)\n",
    "\n",
    "#Estraggo i dati per maggiore leggibilità\n",
    "human_coefficents=[row[2] for row in dataset]\n",
    "wu_coefficents=[row[3] for row in dataset]\n",
    "shortest_coefficents=[row[4] for row in dataset]\n",
    "leak_coefficents=[row[5] for row in dataset]\n",
    "\n",
    "#Calcolo coefficienti\n",
    "correlation_coefficient,p_value = pearsonr(human_coefficents, wu_coefficents)\n",
    "print(\"Perason for Wu Palmer: \",str(correlation_coefficient))\n",
    "correlation_coefficient,p_value = spearmanr(human_coefficents, wu_coefficents)\n",
    "print(\"Spearman rank for Wu Palmer: \",str(correlation_coefficient),\"\\n\")\n",
    "\n",
    "correlation_coefficient,p_value = pearsonr(human_coefficents, shortest_coefficents)\n",
    "print(\"Perason for Shortest Path: \", str(correlation_coefficient))\n",
    "correlation_coefficient,p_value = spearmanr(human_coefficents, shortest_coefficents)\n",
    "print(\"Spearman rank for Shortest Path: \",str(correlation_coefficient),\"\\n\")\n",
    "\n",
    "\n",
    "correlation_coefficient,p_value = pearsonr(human_coefficents, leak_coefficents)\n",
    "print(\"Perason for Leakcock & Chodorow: \", str(correlation_coefficient))\n",
    "correlation_coefficient,p_value = spearmanr(human_coefficents, leak_coefficents)\n",
    "print(\"Spearman rank for Leakcock & Chodorow: \",str(correlation_coefficient),\"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
