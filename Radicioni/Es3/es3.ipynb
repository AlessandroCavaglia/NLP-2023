{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Dato il Trump Twitter Archive acquisire due language model (Bi-grammi e Tri-grammi) e usarli per produrre Tweet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import and constants\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#Utilities\n",
    "def read_csv(filename):\n",
    "    dataset=[]\n",
    "    with open(filename, 'r',encoding='utf-8') as file:\n",
    "        # Create a CSV reader object\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if(not row[1][0]==\"@\"):\n",
    "                sentence_bi = \"[\" + row[1].replace(\"&amp\",\"\") + \"]\"\n",
    "                sentence_tri = \"[[\" + row[1].replace(\"&amp\",\"\") + \"]]\"\n",
    "                #Per ogni tweet aggiungo una versione con un token di start e un token di fine e una versione con due token\n",
    "                #di start e due token di fine da usare per i bi-grammi e i tri-grammi\n",
    "                dataset.append((sentence_bi,sentence_tri))\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Language model a Bi-grammi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#Data una sequenza di token rappresentate tutti i tweet del dataset\n",
    "#Estraggo tutti i bigrammi sotto forma di un dictonary contenente dictonary\n",
    "#Salvando le frequenze di ogni bigramma\n",
    "#Ad esempio per il bigramma (i,eat) sarà salvato in bigram_counts[i] un dictionary tale che bigram_counts[i][eat]=1\n",
    "def count_bigrams(tokens):\n",
    "    bigram_counts = {}\n",
    "    for i in range(len(tokens) - 1):\n",
    "        current_token = tokens[i]\n",
    "        next_token = tokens[i + 1]\n",
    "        if current_token not in bigram_counts:\n",
    "            bigram_counts[current_token] = {}\n",
    "        if next_token not in bigram_counts[current_token]:\n",
    "            bigram_counts[current_token][next_token] = 0\n",
    "        bigram_counts[current_token][next_token] += 1\n",
    "    return bigram_counts\n",
    "\n",
    "\n",
    "def learn_bi_gram_model(dataset):\n",
    "    #Creo un array di token composto dalla sequenza di token di tutti i tweet del dataset\n",
    "    tokens = [nltk.word_tokenize(sentence) for sentence in [elem[0] for elem in dataset]]\n",
    "    tokens = [token for sublist in tokens for token in sublist]\n",
    "    #Conto i bi-grammi\n",
    "    bigram_counts = count_bigrams(tokens)\n",
    "\n",
    "    #Calcolo il numero di parole del vocabolario\n",
    "    vocab_size = len(set(tokens))\n",
    "\n",
    "    #Inserisco le probabilità in biTwitter_probabilities con Laplace Smoothing\n",
    "    biTwitter_probabilities = {}\n",
    "    #Per ogni primo token dei bigrammi\n",
    "    for first_token in bigram_counts:\n",
    "        sum=0\n",
    "        #Calcolo il conteggio di tutti i bigrammi che iniziano con il primo token\n",
    "        for second_token in bigram_counts[first_token]:\n",
    "            sum+=bigram_counts[first_token][second_token]\n",
    "        #Per ogni secondo token nel bigramma\n",
    "        for second_token in bigram_counts[first_token]:\n",
    "            #Creo il bigramma come tupla (Utile dopo per la generazione)\n",
    "            bi_gram=(first_token,second_token)\n",
    "            #Calcolo la probabilità del singolo bigramma\n",
    "            biTwitter_probabilities[bi_gram] = (bigram_counts[first_token][second_token] + 1) / (sum + vocab_size)\n",
    "\n",
    "    return biTwitter_probabilities\n",
    "\n",
    "def generate_text_bigram(prob):\n",
    "    #Uso come token iniziale per la generazione il token di start\n",
    "    current_word = \"[\"\n",
    "    generated_text = [current_word]\n",
    "    #Finchè non genero un token di fine o supero la lunghezza massima\n",
    "    while current_word != \"]\" and len(generated_text) < 50:\n",
    "        #Calcolo i possibili bigrammi da generare selezionandoli sulla base del token attuale\n",
    "        possible_next_bigrams = [bigram for bigram in prob if bigram[0] == current_word]\n",
    "        #Estraggo le probabilità dei prossimi bigrammi\n",
    "        probabilities = [prob[(bigram)] for bigram in possible_next_bigrams]\n",
    "        #Faccio una scelta casuale lavorando sulle probabilità dei bigrammi\n",
    "        next_word = random.choices(possible_next_bigrams, probabilities)[0][1]\n",
    "        generated_text.append(next_word)\n",
    "        current_word = next_word\n",
    "        #Una prima implementazione non faceva una scelta casuale ma prendeva deterministicamente sempre il primo.\n",
    "        #Questa implementazione però ricorreva spesso in loop infiniti ed è stata sotituita con una più casuale\n",
    "    return \" \".join(generated_text[1:-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main e uso del modello a bi-grammi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIGRAM GENERATED SENTENCES\n",
      "The ruling @ UnionLeader . Cruz is a protected 2nd Amendment biggest ) goes to the winners ; losers out of which sadly there are last on Wednesday January 17th rather be weak ; losers refuse to admire about Doral bedbugs but the nickname Mini Mike was biggest\n",
      "Another attack in Madison Square Garden during my best wishes to watch Celebrity @ AGSchneiderman ’ s phony last on being on Tariffs ; prosper be our rapidly rebuilding Military Vets ( cont ) picking winners for his dumb as National Magazine and losers of the haters and\n",
      "I have it ...\n"
     ]
    }
   ],
   "source": [
    " # Lettura CSV\n",
    "file_path = 'trump_twitter_archive/tweets.csv'\n",
    "df = read_csv(file_path)\n",
    "\n",
    "bi_prob=learn_bi_gram_model(df)\n",
    "print(\"BIGRAM GENERATED SENTENCES\")\n",
    "print(generate_text_bigram(bi_prob))\n",
    "print(generate_text_bigram(bi_prob))\n",
    "print(generate_text_bigram(bi_prob))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Language model a Tri-grammi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "#Data una sequenza di token rappresentate tutti i tweet del dataset\n",
    "#Estraggo tutti i trigrammi sotto forma di un dictonary con chiave un bigramma contenente dictonary\n",
    "#Salvando le frequenze di ogni trigramma\n",
    "#Ad esempio per il trigramma (i,eat,ramen) sarà salvato in trigram_counts[(i,eat)] un dictionary tale che bigram_counts[(i,eat)][ramen]=1\n",
    "def count_trigrams(tokens):\n",
    "    trigram_counts = {}\n",
    "    for i in range(len(tokens) - 2):\n",
    "        current_token = tokens[i]\n",
    "        next_token = tokens[i + 1]\n",
    "        next_next_token = tokens[i + 2]\n",
    "        if (current_token, next_token) not in trigram_counts:\n",
    "            trigram_counts[(current_token, next_token)] = {}\n",
    "        if next_next_token not in trigram_counts[(current_token, next_token)]:\n",
    "            trigram_counts[(current_token, next_token)][next_next_token] = 0\n",
    "        trigram_counts[(current_token, next_token)][next_next_token] += 1\n",
    "    return trigram_counts\n",
    "\n",
    "def learn_tri_gram_model(dataset):\n",
    "    #Creo un array di token composto dalla sequenza di token di tutti i tweet del dataset\n",
    "    tokens = [nltk.word_tokenize(sentence) for sentence in [elem[1] for elem in dataset]]\n",
    "    tokens = [token for sublist in tokens for token in sublist]\n",
    "    #Conto i tri-grammi\n",
    "    trigram_counts = count_trigrams(tokens)\n",
    "    #Calcolo il numero di parole del vocabolario\n",
    "    vocab_size = len(set(tokens))\n",
    "\n",
    "    #Inserisco le probabilità in triTwitter_probabilities con Laplace Smoothing\n",
    "    triTwitter_probabilities = {}\n",
    "    #Per ogni primo bigramma iniziale dei trigrammi\n",
    "    for first_bigram in trigram_counts:\n",
    "        sum = 0\n",
    "        #Calcolo il conteggio di tutti i tri-grammi che iniziano con il primo bi-gramma\n",
    "        for third_token in trigram_counts[first_bigram]:\n",
    "            sum += trigram_counts[first_bigram][third_token]\n",
    "         #Per ogni terzo token nel bigramma\n",
    "        for third_token in trigram_counts[first_bigram]:\n",
    "            #Creo il tri-gramma\n",
    "            tri_gram = (first_bigram[0],first_bigram[1], third_token)\n",
    "            #Calcolo la probabilità\n",
    "            triTwitter_probabilities[tri_gram] = (trigram_counts[first_bigram][third_token] + 1) / (sum + vocab_size)\n",
    "    return triTwitter_probabilities\n",
    "\n",
    "def generate_text_trigram(prob):\n",
    "    #Uso i token di start come tri-gramma iniziale per la generazione\n",
    "    current_trigram = (\"[\", \"[\", \"[\")\n",
    "    generated_text = list(current_trigram)\n",
    "    #Finchè non genero i token di fine o supero la soglia massima\n",
    "    while current_trigram[-2:] != (\"]\", \"]\") and len(generated_text) < 50:\n",
    "        #Se ho già generato almeno 6 caratteri genero il prossimo trigramma in modo deterministico\n",
    "        if (len(generated_text) > 6):\n",
    "            #Calcolo il trigramma con la probabilità maggiore dato il bigramma attuale\n",
    "            next_trigram = max(prob, key=lambda tri_gram: tri_gram[:2] == current_trigram[-2:])\n",
    "            generated_text.append(next_trigram[-1])\n",
    "            current_trigram = next_trigram\n",
    "        else:\n",
    "            #Genero il prossimo bigramma in modo più casuale in modo da non generare sempre lo stesso tweet\n",
    "            #Calcolo tutti i possibili prossimi trigrammi\n",
    "            possible_next_trigrams = [trigram for trigram in prob if\n",
    "                                      trigram[:2] == current_trigram[-2:]]\n",
    "            #Mappo le loro probabilità\n",
    "            probabilities = [prob[trigram] for trigram in possible_next_trigrams]\n",
    "            #Faccio una selezione casuale basata sulle probabilità\n",
    "            next_trigram = random.choices(possible_next_trigrams, probabilities)[0]\n",
    "            generated_text.append(next_trigram[-1])\n",
    "            current_trigram = next_trigram\n",
    "\n",
    "    return \" \".join(generated_text[3:-2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Uso del modello a tri-grammi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRIGRAM GENERATED SENTENCES\n",
      "Wow I ’ m at 2200000 followers but I did build a world class loser Tim O ’ Brien who I haven ’ t know how to win and their so-called Lincoln Project ” goes into their own pockets . With what I ’ ve\n",
      "Steyer is a LOSER who has money but can ’ t know how to win and their so-called Lincoln Project ” goes into their own pockets . With what I ’ ve done on Judges Taxes Regulations Healthcare the Military Vets ( Choice ! )\n",
      "LOSER ! https : //t.co/p5imhMJqS1\n"
     ]
    }
   ],
   "source": [
    "tri_prob=learn_tri_gram_model(df)\n",
    "print(\"\\nTRIGRAM GENERATED SENTENCES\")\n",
    "print(generate_text_trigram(tri_prob))\n",
    "print(generate_text_trigram(tri_prob))\n",
    "print(generate_text_trigram(tri_prob))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
