{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Summarization estrattiva con Nasari\n",
    "File scelto, trump-wall\n",
    "# Scelta implementativa importante:\n",
    "Per evitare il paywall si è deciso di provare ad evitare l'uso di BableNet e di utilizzare come chiave di Nasari il lemma presente nella seconda colonna del file.\n",
    "Si sono raggiunte comunque buoni risultati"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import and constants\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "COMPRESSION_RATE=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#Utilities\n",
    "\n",
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def load_nasari(filename):\n",
    "    nasari={}\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line=line.lower()\n",
    "            line = line.strip().split(';')\n",
    "            nasari[line[1]] = []\n",
    "            for elem in line[2:]:\n",
    "                elem = elem.strip().split('_')\n",
    "                if len(elem) == 2:\n",
    "                    nasari[line[1]].append((elem[0],float(elem[1])))\n",
    "    return nasari\n",
    "#Calcola la misura di valutazione Rouge tra il reference e il generated summary\n",
    "def calculate_rouge(reference_summary, generated_summary):\n",
    "    reference = set(word_tokenize(reference_summary))\n",
    "    generated = set(word_tokenize(generated_summary))\n",
    "    blue_score = len(reference.intersection(generated)) / len(reference)\n",
    "    return blue_score\n",
    "\n",
    "#Calcola la misura di valutazione Blue tra il reference e il generated summary\n",
    "def calculate_blue(reference_summary, generated_summary):\n",
    "    reference = set(word_tokenize(reference_summary))\n",
    "    generated = set(word_tokenize(generated_summary))\n",
    "    blue_score = len(reference.intersection(generated))/len(generated)\n",
    "    return blue_score\n",
    "\n",
    "#Conta le frasi di un testo\n",
    "def count_sentences(text):\n",
    "    sentence_tokens = sent_tokenize(text)\n",
    "    num_sentences = len(sentence_tokens)\n",
    "    return num_sentences\n",
    "\n",
    "#Calcola l'overlap  tra due vettori pesato per il loro valore di peso\n",
    "def calculate_weighted_overlap(vec1,vec2):\n",
    "    overlap=0\n",
    "    for elem_1 in vec1:\n",
    "        for elem_2 in vec2:\n",
    "            if(elem_1[0]==elem_2[0]):\n",
    "                overlap+=elem_1[1]+elem_2[1]\n",
    "    return overlap\n",
    "#Lemmatizza un testo rimuovendo le stopWords\n",
    "def lemmatize_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Funzione di summarization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#Per capire la similarità di due parole calcoliamo il weighted overlap tra i loro embedding di Nasari\n",
    "def get_word_similarity(word1, word2,nasari):\n",
    "    try:\n",
    "        vec1 = nasari[word1]\n",
    "        vec2 = nasari[word2]\n",
    "        return calculate_weighted_overlap(vec1, vec2)\n",
    "    except:\n",
    "        #print(word1,\"-\",word2)\n",
    "        return 0.0\n",
    "\n",
    "#Forniamo uno score ad una frase sulla base della similarità dei token della frase ai keyword tokens\n",
    "def calculate_scores(text_tokens, keyword_tokens,nasari):\n",
    "    scores = []\n",
    "    #Per ogni token nella frase tokenizzata\n",
    "    for token in text_tokens:\n",
    "        max_similarity = 0\n",
    "        #Per ogni parola nei keyword_tokens\n",
    "        for keyword in keyword_tokens:\n",
    "            #Calcolo la similarità\n",
    "            similarity = get_word_similarity(token, keyword,nasari)\n",
    "            #Eventualmente aggiorno il massimo\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "        #Aggiungo nella lista degli scores delle parole la similarità massima\n",
    "        scores.append(max_similarity)\n",
    "    #Ritorno la somma degli scores di tutte le parole\n",
    "    return sum(scores)\n",
    "\n",
    "def summarize_text(text, num_sentences,nasari):\n",
    "    #Lemmatizzo il testo e rimuovo le stopwrods per ottenere le keyword del testo\n",
    "    keyword_tokens = lemmatize_text(text)\n",
    "    #Divido il testo in frasi\n",
    "    sentence_tokens = sent_tokenize(text)\n",
    "    sentence_scores = []\n",
    "    #Per ogni frase\n",
    "    for index,sentence in enumerate(sentence_tokens):\n",
    "        #Lemmatizzo la frase\n",
    "        tokens = lemmatize_text(sentence)\n",
    "        #Calcolo il punteggio della frase\n",
    "        sentence_score = calculate_scores(tokens, keyword_tokens,nasari)\n",
    "        #Salvo il punteggio\n",
    "        sentence_scores.append((index,sentence_score))\n",
    "    #Ordino le frasi sulla base del loro score\n",
    "    sorted_indices = sorted(sentence_scores, key=lambda x: x[1],reverse=True)\n",
    "    #Estraggo i primi  num_sentences indici\n",
    "    top_indices = sorted_indices[:num_sentences]\n",
    "    #Estraggo le frasi relative agli indici\n",
    "    summary_sentences = [sentence_tokens[i] for i in (index[0] for index in top_indices)]\n",
    "    #Creo il summary\n",
    "    summary = ' '.join(summary_sentences)  # Unisci le frasi per ottenere il riassunto\n",
    "    return summary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main e valutazione"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Carico Nasari\n",
    "nasari = load_nasari(\"Nasari/dd-small-nasari-15.txt\")\n",
    "#Carico il dataset\n",
    "dataset_text = read_text_from_file(\"Nasari/Trump-wall.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION: 0.976148409893993\n",
      "RECALL: 0.9364406779661016\n"
     ]
    }
   ],
   "source": [
    "#Carico il gold-set\n",
    "reference_text = read_text_from_file(\"Nasari/Trump-wall-reference-80.txt\")\n",
    "#Calcolo il numero di frasi che voglio estrarre\n",
    "num_phrases = int(count_sentences(dataset_text) * (1 - COMPRESSION_RATE / 100))\n",
    "#Calcolo il summary\n",
    "generated_summary = summarize_text(dataset_text,num_phrases,nasari)\n",
    "\n",
    "#Calcolo gli scores\n",
    "rouge_scores = calculate_rouge(reference_text, generated_summary)\n",
    "blue_score = calculate_blue(reference_text, generated_summary)\n",
    "\n",
    "print(\"PRECISION:\",blue_score)\n",
    "print(\"RECALL:\",rouge_scores)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
